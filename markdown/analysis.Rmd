---
title: "Analyzing the German Operating Reserve Market"
output: html_notebook
fig_width: 6 
fig_height: 4
---




## Data Mining Process

For the following analysis we will apply the CRISP-DM process as seen in the image below. First it is important to precisely define the goal and problem set. Therefore one has to understand the business needs and processes involved. Based on that, the necessary data can be acquired and prepared for the model building step.


![1. CRISP-DM Process](/Users/Timo/Documents/Studium/2.\ Master/Masterarbeit/\[3\]_Ausarbeitung/LaTex/GER_version/1_graphics/crisp_dm.png){ width=42% }


## Busines Understanding

### Determine Business Objectives

First, to understand the purpose and to be able to interpret the results of the analysis, background information of the business domain is provided in the following section. Hereby, the business objectives for the analysis are defined as well as success criterias to meassure the fulfillment of the goals.

#### Background


##### Secondary Reserve Power

The German Reserve Market distinguishes between three different product types (19/24 european markets follow this pattern [source](http://www.strommarkttreffen.org/2016-1-22-Ocker-German-Secondary-Balancing-Power-Market.pdf.pdf)):

- **Primary Reserve power (PR)**
- **Secondary Reserve Power (SR)**
- **Tertiary Reserve Power (TR)**

Their differences concern the time aspect of power delivery. The image underneath shows the activation duration for full power and the activation time when full power is available. 


![2. The 'three-quality' pattern (PR/SR/TR) in the German Reserve Market](/Users/Timo/Documents/Studium/2.\ Master/Masterarbeit/\[3\]_Ausarbeitung/LaTex/GER_version/1_graphics/rl_timeline.png){ width=50% }

The focus of this analysis lies on the Secondary Reserve Power since its activation time is reasonable for common energy suppliers. The market for SR is also the biggest. 




##### German Reserve Market

Along with the ... law in 2009 the four german TSOs joined and control the German Reserve Market. Their duty for transparency led to the plattform [regelleistung.net](https://www.regelleistung.net). There all data around the reserve market is published. Also the auction results of the SR. Those auctions are held weeklys from monday till wednesday. The offers account for one week (monday till sunday) and are divided in main period and sub period as well as negative and positive SR. An offer consists of a power price, a work price and the offered SR (negative or positive and its amount in MW). 




##### Scenario: Facility 



#### Objective

The overall aim is to give a more precise estimation for the call probability of working prices of the secondary reserve power. Those probabilities can be used as an input paramter for an optimization algorithm which controlls the energy purchasing and selling flow in both primary energy market and reserve energy market. By now the optimal working prices are set to exceed the contribution margin.


#### Success Criterias




### Assess situation

Satus Quo, Resources, Requirements, Assumptions, Constraints, Terminology







### Determine Data mining Goals

Goals, success criterias









### Data Understanding

Next, we will focus on the understanding of the data. As mentioned previously, the main source is the transparent internet platform [regelleistung.net](https://www.regelleistung.net)

#### Collect Data

A R package exists `rmarketcrawlR` accessable via [github](https://github.com/wagnertimo/rmarketcrawlR) which allows crawiling the platform. It also provides additional functions to approximate minutely secondary reserve calls and hence calculate marginal work prices. 
To get started with the package use `devtools` and its `Ã¬nstall_github()` function.

```{r}
library(devtools)
install_github("wagnertimo/rmarketcrawlR")
```

##### Example Data of 2016

Than you are able to activate the library and aquire the secondary reserve powerd data. We will focus on the year 2016 and get the results of the auctions, the 15 minute reserve calls and the 4 seconds reserve needs. The values rely to the *Netzregelverbund* , a composition of the four german TSOs. After all the data is crawled from the internet, the marginal work prices can be computed by approximating minutely reserve calls.

```{r}
# Activate the package in the workspace
library(rmarketcrawlR)

# Set logging true to be able to comprehend the steps or errors
setLogging(TRUE)

# Get the data of 2016 for operating needs, calls and auctions of secondary reserve power from the Netzregelverbund
needs.2016 = getReserveNeeds('01.01.2016', '31.01.2016')
calls.2016 = getReserveCalls('01.01.2016', '31.01.2016', '6', 'SRL')
auctions.2016 = getReserveAuctions('01.01.2016', '31.01.2016', '2')

# Calculate the marginal work prices for 2016 with 2 cores --> This step takes a while
mwp.2016 = getMarginalWorkPrices(needs.2016, calls.2016, auctions.2016, numCores = 2)
```


> For further information about the  `rmarketcrawlR` package read the `README.md` and the documentation.

##### The complete Data Set of the Analysis

This analaysis will be based on the data of the years from 2011 till 2016 (`2011-07-01 00:00:00 CEST` till `2016-12-31 23:59:00 CET`). These are 5 and a half years of data. The commands below will crawl for the observations. Pre-processing operations to prepare the raw data for analysis is described in the sections below.

```{r}
# Get raw input data
needs.2016 = getReserveNeeds('01.01.2016', '31.12.2016')
calls.2016 = getReserveCalls('01.01.2016', '31.12.2016', '6', 'SRL')
auctions.2016 = getReserveAuctions('01.01.2016', '31.12.2016', '2')

needs.2015 = getReserveNeeds('01.01.2015', '31.12.2015')
calls.2015 = getReserveCalls('01.01.2015', '31.12.2015', '6', 'SRL')
auctions.2015 = getReserveAuctions('01.01.2015', '31.12.2015', '2')

needs.2014 = getReserveNeeds('01.01.2014', '31.12.2014')
calls.2014 = getReserveCalls('01.01.2014', '31.12.2014', '6', 'SRL')
auctions.2014 = getReserveAuctions('01.01.2014', '31.12.2014', '2')


needs.2013 = getReserveNeeds('01.01.2013', '31.12.2013')
calls.2013 = getReserveCalls('01.01.2013', '31.12.2013', '6', 'SRL')
auctions.2013 = getReserveAuctions('01.01.2013', '31.12.2013', '2')


needs.2012 = getReserveNeeds('01.01.2012', '31.12.2012')
calls.2012 = getReserveCalls('01.01.2012', '31.12.2012', '6', 'SRL')
auctions.2012 = getReserveAuctions('01.01.2012', '31.12.2012', '2')


needs.2011 = getReserveNeeds('01.07.2011', '31.12.2011')
calls.2011 = getReserveCalls('01.07.2011', '31.12.2011', '6', 'SRL')
auctions.2011 = getReserveAuctions('01.07.2011', '31.12.2011', '2')

```



#### Describe Data

##### Data Sets and Features


**WHY NETZREGELVERBUND DATA???**

##### Outliers and Missing values

Before we can use the collected raw data for pre-processing it to calculate the marginal work prices and other computations based on that, the data must be cleaned. It is neccessary to handle missing data and outliers. Both has been encountered. Outliers were identified for the operative reserve needs in 2011 and 2012 (data.frames `needs.2011` and `needs.2012`). Those oultliers are characterized by continuous zero values over a longer period of observations. This radical change of reserve power needs is interpreted as an outlier (perhaps a meassuring error).

On the other hand there are some missing values for operative reserve needs in 2012 (data.frame `needs.2012`) and the operative reserve calls in 2011, 2012 and 2013 (data.frames `calls.2011`, `calls.2012` and `calls.2013`). The operative needs values in 2012 show *NaN* values for a few specific dates. Also some reserve calls (negative and positive) of the *Netzregelverbund* are missing in 2011, 2012, and 2013.

All those data.frames have to be pre-processed and cleaned. The procedures are described in the next sections.

> If you want to achieve the same analysis results, it is important to follow these steps since this is the data base of all calculations. It is also important to mention that without treating those outliers and missing (or NaN) values, the `getMarginalWorkPrices()` or `getOneMinuteCalls()` will not work with the compromised data sets.



##### Zero values for operative reserve needs

By investigating the operative reserve needs for 2011 and 2012 there have been outliers detected. There is a constant row of zero values for the reserve needs (in MW) starting at `2011-07-26 11:06:40` till `2011-07-26 12:03:32` (with an exceptional value of *-1221.1385* at `2011-07-26 12:03:00`). This zero row is preceeded and followed by negative reserve needs around the thousand MW area. There are also zeros in 2012 for `2012-08-07 10:19:48` till  (excluding) `2012-08-07 11:33:12`. The abrupt decline and long holding of those zero rows indicates outliers (both sources https://www.transnetbw.de/de/strommarkt/systemdienstleistungen/regelenergie-bedarf-und-abruf#srlbedarf2011 and http://www.50hertz.com/de/Maerkte/Regelenergie/Regelenergie-Downloadbereich show outliers).


```{r}

# function to get the zero outliers
checkForZeroOutliers <- function(df) {
  # Cut the 4sec data in 15 minute blocks to count 0 values for all blocks
  df$cuttedTime <- cut(df$DateTime, breaks = paste("15", "min", sep = " "))
  df$cuttedTime <- as.POSIXct(df$cuttedTime, tz = "Europe/Berlin")
  # Create a variable to mark a zero MW value
  df$count <- ifelse(needs.2011$MW == 0, 1,0)
  
  # Count the zero markers by summing for every 15min block
  r <- df %>%
    group_by(cuttedTime) %>%
    summarise(n= sum(count)) %>%
    filter(n == 225)
  
  return(r)
}


zeroCounts <- checkForZeroOutliers(needs.2011)
head(zeroCounts)

# Output:
#            cuttedTime     n
# 1 2011-07-26 11:15:00   225
# 2 2011-07-26 11:30:00   225
# 3 2011-07-26 11:45:00   225

zeroCounts <- checkForZeroOutliers(needs.2012)
head(zeroCounts)
#            cuttedTime     n
# 1 2012-08-07 10:30:00   225
# 2 2012-08-07 10:45:00   225
# 3 2012-08-07 11:00:00   225
# 4 2012-08-07 11:15:00   225
```

To replace or impute those zero values we could simply take the data of the (two) year before on `2010-07-26` (`2012-08-07`). I will show the replacement procedure for the data of 2011. Therefore we will compare  some statistics with the observations of an hour before and after the outliers. 

```{r}

# Get oprative reserve needs of the year before
n.2010 = getReserveNeeds('26.07.2010', '26.07.2010')

n.2011.hour.before <- needs.2011[needs.2011$DateTime >= as.POSIXct("2011-07-26 10:00:00") & needs.2011$DateTime < as.POSIXct("2011-07-26 11:00:00"),]
n.2011.hour.after <- needs.2011[needs.2011$DateTime >= as.POSIXct("2011-07-26 12:00:00") & needs.2011$DateTime < as.POSIXct("2011-07-26 13:00:00"),]
n.2010.hour.at <- n.2010[n.2010$DateTime >= as.POSIXct("2010-07-26 11:06:40") & n.2010$DateTime < as.POSIXct("2010-07-26 12:03:32"),]

summary(n.2011.hour.before$MW)
#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# -1578.1  -961.5  -753.8  -795.6  -600.0  -270.6 
   
summary(n.2011.hour.after$MW)
#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# -1597.5 -1154.6  -910.7  -881.0  -697.7     0.0 

summary(n.2010.hour.at$MW)
#     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
# -1859.19  -284.38   -72.02  -139.44    60.17   379.32  

```

The statistics show that the distribution is quite different. The mean and median is lower. There are also positive reserve needs in 2010. To gain a better insight of the distribution, we will plot the data with the outliers and then with the replaced 2010 data. The figure in the code snippet below shows the outliers.

```{r}
# hour before and after the hour with the zero values
n.2011.hour <- needs.2011[needs.2011$DateTime >= as.POSIXct("2011-07-26 10:00:00") & needs.2011$DateTime < as.POSIXct("2011-07-26 13:00:00"),]

# Plot the outliers in context of the hour before and after
library(ggplot2)
ggplot(data = n.2011.hour, aes(x=DateTime, y = MW)) +
  geom_line() +
  labs(title="Zero Outliers For Opreative Reserve Needs on 2011-07-26")
```

Now it is time to fit in the 2010 data.

```{r}
# Replace the zero values with the data of the same time period in 2010
n.2011.hour[n.2011.hour$DateTime >= as.POSIXct("2011-07-26 11:06:40") & n.2011.hour$DateTime < as.POSIXct("2011-07-26 12:03:32"),]$MW <- n.2010[n.2010$DateTime >= as.POSIXct("2010-07-26 11:06:40") & n.2010$DateTime < as.POSIXct("2010-07-26 12:03:32"),]$MW

# Plot the replaced data
ggplot(data = n.2011.hour, aes(x=DateTime, y = MW)) +
  geom_line() +
  labs(title="Replaced Zero Outliers With Opreative Reserve Needs of 2010")
```

At the begining there is a spike from around *-1000* to *-300* and then the 2010 data falls back to around *-1800*. This indicates some turbulances. One could argue and try to make this transition more smoothly. So we will modify the data a bit to smooth the extreme values and to get closer to the statistics (see in the section above) of the hours before and after. 

```{r}
# Replace the zero values with the data of the same time period in 2010
n10 <- n.2010[n.2010$DateTime >= as.POSIXct("2010-07-26 11:06:40") & n.2010$DateTime < as.POSIXct("2010-07-26 12:03:32"),]
n10$MW <- ifelse(n10$MW >= -1200, n10$MW - 400, n10$MW + 400)
summary(n10$MW)


n.2011.hour[n.2011.hour$DateTime >= as.POSIXct("2011-07-26 11:06:40") & n.2011.hour$DateTime < as.POSIXct("2011-07-26 12:03:32"),]$MW <- n10$MW

# Plot the modified replacing data
ggplot(data = n.2011.hour, aes(x=DateTime, y = MW)) +
  geom_line() +
  labs(title="Replaced Zero Outliers With Modified Operative Reserve Needs of 2010")
```

This looks quite smooth. It captures the uptrend and downtrend at around `11:30:00` and also the unique spike at the begining. The whole series lies below zero. Now we will replace those values with the original data set of the operative reserve needs in 2011 (`needs.2011`).


```{r}

# Replace the 0 data (outliers) in the 2011 data set with the values of 2010
needs.2011[needs.2011$DateTime >= as.POSIXct("2011-07-26 11:06:40") & needs.2011$DateTime < as.POSIXct("2011-07-26 12:03:32"),]$MW <- n10$MW

# Sanity check that outliers are removed
head(checkForZeroOutliers(needs.2011))
# 0 rows
```

The same was done with the data of 2012. The graphic below shows the fitted *hole*. The data of 2010 was modified by reversing its order (such that it followed the uptrend) and adding 400 MW.

![Replacing zero outliers values for 2017-08-07](zero_07_08.png){ width=100% }



##### NaN values for operative reserve needs in 2012

Both sources https://www.transnetbw.de/de/strommarkt/systemdienstleistungen/regelenergie-bedarf-und-abruf#srlbedarf2011 and http://www.50hertz.com/de/Maerkte/Regelenergie/Regelenergie-Downloadbereich show NaN values on the dates `2012-07-04` `2012-07-05` `2012-07-06` and `2012-07-07`.

```{r}
nans <- filter(needs.2012, MW == "NaN")
unans <- unique(format(nans$DateTime, "%Y-%m-%d"))
unans
# "2012-07-04" "2012-07-05" "2012-07-06" "2012-07-07"

# Side Note:
# Time periods for the NaN values
#
# "2012-07-04 21:45:04 CEST" - "2012-07-04 21:50:36 CEST"
# "2012-07-05 21:45:08 CEST" - "2012-07-05 22:01:12 CEST"
# "2012-07-06 21:45:04 CEST" - "2012-07-06 21:52:12 CEST"
# "2012-07-07 21:45:04 CEST" - "2012-07-07 21:50:52 CEST"

```


Next we will look at the data with the NaN values for one (`2012-07-05`) of the four dates. This shows the approach how the rest will be replaced with the data of the year 2010. The data of 2011 is not taken, since this could bias the analysis as it is part of the whole data set. Optional, some values will be adjusted to fit more smoothly into the time series. Therefore we will visualize the time series data with a few values before and after the NaN values depending on the range of the NaN values.
```{r}
# Get the needed data of 2010
n.2010 <- getReserveNeeds("04.07.2010","07.07.2010")
# set the date, e.g. 2012-07-05
date <- 2 
# get the time period of the NaN values
mini <- min(nans[format(nans$DateTime, "%Y-%m-%d") == as.Date(unans[date]), "DateTime"])
maxi <- max(nans[format(nans$DateTime, "%Y-%m-%d") == as.Date(unans[date]), "DateTime"])

mi <- mini - (maxi - mini) # get the date for a few minutes before
ma <- maxi + (maxi - mini) # get the date for a few minutes after


# get the data a few minutes (range of max and min) before and after the NaN values
n <- needs.2012[needs.2012$DateTime >= mi & needs.2012$DateTime <= ma,]

```


Now let us plot the time series data with the NaN *hole* which has to be filled with the data of 2010.
```{r}
# plot the data with the NaN hole
ggplot(data = n, aes(x=DateTime, y = MW)) +
  geom_line() +
  labs(title = paste("NaN Values For Operative Reserve Needs on ", unans[date], sep=""))

```

The next step is to get the data of 2010 and replace the NaN values.
```{r}
# get the dates of the last year
miny <- seq(mini, length=2, by="-2 years")[2]
maxy <- seq(maxi, length=2, by="-2 years")[2]
# get the data of the last year
nr <- n.2010[n.2010$DateTime >= miny & n.2010$DateTime <= maxy,]

# replace the NaN values with that of the last year
n[n$DateTime >= mini & n$DateTime <= maxi,]$MW <- nr$MW
```


Again, the plot with the replaced data gives us an insight on how well the data fits into the time series.
```{r}
# plot the data with the filled hole
ggplot(data = n, aes(x=DateTime, y = MW)) +
  geom_line() +
  labs(title = paste("Replaced NaN Values For Operative Reserve Needs on ", unans[date], sep=""))
```

Here, the data in 2010 seems to be a bit low and the transitions at the end is a bit too steep. Hence, we will adjust those values by adding a constant value of 200 MW.
```{r}
# Optional!!
# Modify and adjust the replacing values
nr$MW <- nr$MW + 200

# replace the adjusted values with that of the last year
n[n$DateTime >= mini & n$DateTime <= maxi,]$MW <- nr$MW
```


This time the data fits more smoothly. There is still a steep ascent of reserve needs at the begining and end, but this is acceptable.
```{r}
# plot the data with the filled hole
ggplot(data = n, aes(x=DateTime, y = MW)) +
  geom_line() +
  labs(title = paste("Adjusted Replace Values For Operative Reserve Needs on ", unans[date], sep=""))
```

If the modification is enough, the data (`nr` data.frame holds the replacing values) can be replaced for the *original* operative needs data set of 2012.
```{r}
# replace NaN VALUES OF the original data set needs.2012
needs.2012[needs.2012$DateTime >= mini & needs.2012$DateTime <= maxi,]$MW <- nr$MW
```

The following graphs show the respective replacement for the other three dates. There were also some adjustment made:

- For `2012-07-04`: `nr$MW = nr$MW - 900`
- For `2012-07-06`: `nr$MW = ifelse(nr$MW < -1250, nr$MW + 800, nr$MW + 300)`
- For `2012-07-07`: `nr$MW = nr$MW + 1200`

![Replacing NaN values for 2017-07-04](nan_04.png){ width=100% }

![Replacing NaN values for 2017-07-06](nan_06.png){ width=100% }

![Replacing NaN values for 2017-07-07](nan_07.png){ width=100% }




##### Missing Values for operative reserve calls

Missing values occur for 15 minute calls of negative and positive SR of the *Netzregelverbund* in 2013, 2012 and 2011:

- **2013:** There are 37 observations in 2013 with missing values. This is around 0.11%. The main date is `2013-12-04`. At `15:45` till the end of the day values for negative and positive SR calls are missing. Then there are four other days (`2013-04-03`, `2013-06-04`, `2013-09-18` and `2013-12-07`) with one observation each.

- **2012:** There are 38 observations in 2012 with missing values. This is around 0.11%. The main dates are `2012-02-26` and `2012-10-27`. with missing values for negative and positive SR calls in the evening and morning hours. Then there are two other days (`2012-08-05`, `2012-09-09`) with one observation each.

- **2011:** There are 235 observations in 2011 (only half the year, starting from `2011-07-01`) with missing values. This is around 1.33%. The dates are too much to list them.



```{r}
# FOR 2013
nrow(calls.2013[is.na(calls.2013$neg_MW), ])
# [1] 37 --> ca. 0.11%
calls.2013[is.na(calls.2013$neg_MW), "DateTime"]
# [1] "2013-04-03 19:00:00 CEST" "2013-06-04 15:00:00 CEST"
# [3] "2013-09-18 08:30:00 CEST" "2013-12-04 15:45:00 CET" 
# [5] "2013-12-04 16:00:00 CET"  "2013-12-04 16:15:00 CET" 
# [7] "2013-12-04 16:30:00 CET"  "2013-12-04 16:45:00 CET" 
# [9] "2013-12-04 17:00:00 CET"  "2013-12-04 17:15:00 CET" 
# [11] "2013-12-04 17:30:00 CET"  "2013-12-04 17:45:00 CET" 
# [13] "2013-12-04 18:00:00 CET"  "2013-12-04 18:15:00 CET" 
# [15] "2013-12-04 18:30:00 CET"  "2013-12-04 18:45:00 CET" 
# [17] "2013-12-04 19:00:00 CET"  "2013-12-04 19:15:00 CET" 
# [19] "2013-12-04 19:30:00 CET"  "2013-12-04 19:45:00 CET" 
# [21] "2013-12-04 20:00:00 CET"  "2013-12-04 20:15:00 CET" 
# [23] "2013-12-04 20:30:00 CET"  "2013-12-04 20:45:00 CET" 
# [25] "2013-12-04 21:00:00 CET"  "2013-12-04 21:15:00 CET" 
# [27] "2013-12-04 21:30:00 CET"  "2013-12-04 21:45:00 CET" 
# [29] "2013-12-04 22:00:00 CET"  "2013-12-04 22:15:00 CET" 
# [31] "2013-12-04 22:30:00 CET"  "2013-12-04 22:45:00 CET" 
# [33] "2013-12-04 23:00:00 CET"  "2013-12-04 23:15:00 CET" 
# [35] "2013-12-04 23:30:00 CET"  "2013-12-04 23:45:00 CET" 
# [37] "2013-12-07 20:00:00 CET"

# FOR 2012
nrow(calls.2012[is.na(calls.2012$neg_MW), ])
# [1] 38 --> ca. 0.11%
calls.2012[is.na(calls.2012$neg_MW), "DateTime"]
# [1] "2012-02-26 20:30:00 CET"  "2012-02-26 20:45:00 CET" 
# [3] "2012-02-26 21:00:00 CET"  "2012-02-26 21:15:00 CET" 
# [5] "2012-02-26 21:30:00 CET"  "2012-02-26 21:45:00 CET" 
# [7] "2012-02-26 22:00:00 CET"  "2012-02-26 22:15:00 CET" 
# [9] "2012-02-26 22:30:00 CET"  "2012-02-26 22:45:00 CET" 
# [11] "2012-02-26 23:00:00 CET"  "2012-02-26 23:15:00 CET" 
# [13] "2012-02-26 23:30:00 CET"  "2012-02-26 23:45:00 CET" 
# [15] "2012-08-05 00:00:00 CEST" "2012-09-09 00:00:00 CEST"
# [17] "2012-10-27 00:00:00 CEST" "2012-10-27 00:15:00 CEST"
# [19] "2012-10-27 00:30:00 CEST" "2012-10-27 00:45:00 CEST"
# [21] "2012-10-27 01:00:00 CEST" "2012-10-27 01:15:00 CEST"
# [23] "2012-10-27 01:30:00 CEST" "2012-10-27 01:45:00 CEST"
# [25] "2012-10-27 02:00:00 CEST" "2012-10-27 02:15:00 CEST"
# [27] "2012-10-27 02:30:00 CEST" "2012-10-27 02:45:00 CEST"
# [29] "2012-10-27 03:00:00 CEST" "2012-10-27 03:15:00 CEST"
# [31] "2012-10-27 03:30:00 CEST" "2012-10-27 03:45:00 CEST"
# [33] "2012-10-27 04:00:00 CEST" "2012-10-27 04:15:00 CEST"
# [35] "2012-10-27 04:30:00 CEST" "2012-10-27 04:45:00 CEST"
# [37] "2012-10-27 05:00:00 CEST" "2012-10-27 05:15:00 CEST"

# FOR 2011
nrow(calls.2011[is.na(calls.2011$neg_MW), ])
# [1] 235 --> ca. 1.33%
calls.2011[is.na(calls.2011$neg_MW), "DateTime"]
# (..)

```


**Impute Missing Values**

For time series analysis on such a data set, it is not appropriate to leave them out. The amount of proportion is quite less, but nevertheless it is better to approximate the missing values. There would be three different approaches:

- **Use the data of other years:** Since there is no intersection with the missing dates, it is possible to impute the missing values with the values of the year before. Only problem is that for 2011 (also for february 2012) no data of 2010 (february 2011) is available. Another option would be to take an average of the *future* data of the years after.
- **Use reserve needs:** It is also possible to use the 4sec reserve needs which are also used for the minutely approximation of the calls.
- **Use the data of the TSOs:** On the given date of missing values, some TSO show SR call data. So one approach would be to sum up those observations and use it to impute the missing values for the *Netzregelverbund*. 


We will go for the latter. Taking the reserve needs would cause a big deviation, since the needs and actual calls differ remarkably. Averaging future years would lead to misinterpretations. Using TSO data seems natural because the summation of the four yield into the *Netzregelverbund* variable. It is also noticed that only one (only for 2011 at `2011-10-30 23:00:00` for one hour 3 TSO lack with data) TSO has missing values for the dates of interest which should keep the error and a possible bias small. This leaves the question how to impute the missing value of that TSO. A simple approach would be to take the value of the day before. This is possible because the first observation has no missing value(`2011-07-01`). The code sample below imputes the data for the already downloaded `data.frames` `calls.2013` `calls.2012` and `calls.2011`.

```{r}
# Use the call data of the 4 TSOs to impute the missing values of the calls (Netzregelverbund) in 2013, 2012 and 2011

# Load the TSO data
# 50Hz (4)
calls.2013.4 = getReserveCalls('01.01.2013', '31.12.2013', '4', 'SRL')
calls.2012.4 = getReserveCalls('01.01.2012', '31.12.2012', '4', 'SRL')
calls.2011.4 = getReserveCalls('01.07.2011', '31.12.2011', '4', 'SRL')
# TenneT (2)
calls.2013.2 = getReserveCalls('01.01.2013', '31.12.2013', '2', 'SRL')
calls.2012.2 = getReserveCalls('01.01.2012', '31.12.2012', '2', 'SRL')
calls.2011.2 = getReserveCalls('01.07.2011', '31.12.2011', '2', 'SRL')
# Amprion (3)
calls.2013.3 = getReserveCalls('01.01.2013', '31.12.2013', '3', 'SRL')
calls.2012.3 = getReserveCalls('01.01.2012', '31.12.2012', '3', 'SRL')
calls.2011.3 = getReserveCalls('01.07.2011', '31.12.2011', '3', 'SRL')
# TransnetBW (1)
calls.2013.1 = getReserveCalls('01.01.2013', '31.12.2013', '1', 'SRL')
calls.2012.1 = getReserveCalls('01.01.2012', '31.12.2012', '1', 'SRL')
calls.2011.1 = getReserveCalls('01.07.2011', '31.12.2011', '1', 'SRL')

tso.list.2013 <- list(calls.2013.1,calls.2013.2,calls.2013.3,calls.2013.4)
tso.list.2012 <- list(calls.2012.1,calls.2012.2,calls.2012.3,calls.2012.4)
tso.list.2011 <- list(calls.2011.1,calls.2011.2,calls.2011.3,calls.2011.4)

#
# Impute missing calls with TSO data
#
imputed.calls.2013 <- imputeMissingCallsWithTSO(calls.2013, tso.list.2013)
imputed.calls.2012 <- imputeMissingCallsWithTSO(calls.2012, tso.list.2012)
imputed.calls.2011 <- imputeMissingCallsWithTSO(calls.2011, tso.list.2011)

# Sanity check that no NA are left
imputed.calls.2013[is.na(imputed.calls.2013$neg_MW), ]
imputed.calls.2012[is.na(imputed.calls.2012$neg_MW), ]
imputed.calls.2011[is.na(imputed.calls.2011$neg_MW), ]

```
> Further references in the analysis to `calls.2013` `calls.2012` and `calls.2011` contain the imputed data sets.




##### Approximation of minutely calls




##### Calculation of marginal work prices and call probabilities






#### Explore Data

##### Auctions Data

Firstly, we will investigate the auction results for the SR in 2016. Since we are interested in the work prices, we are going to plot the average work price bid for each week seperated by main and sub period as well as negative and positive SR.

```{r}
# filter by direction
# and get the average of the work price offers
auctions.2016 %>%
  group_by_(.dots = c("date_from", "Direction")) %>%
  summarise(avg = mean(work_price)) %>%
  ggplot(aes(x = date_from, y = avg, colour = Direction)) + 
        geom_line()

```



##### Marginal Work Prices

```{r}
# Calculate the marginal work prices from the preprocessed raw input data (see sections above)
mwp.2016 = getMarginalWorkPrices(needs.2016, calls.2016, auctions.2016, numCores = 2)
mwp.2015 = getMarginalWorkPrices(needs.2015, calls.2015, auctions.2015, numCores = 2)
mwp.2014 = getMarginalWorkPrices(needs.2014, calls.2014, auctions.2014, numCores = 2)
mwp.2013 = getMarginalWorkPrices(needs.2013, calls.2013, auctions.2013, numCores = 2)
mwp.2012 = getMarginalWorkPrices(needs.2012, calls.2012, auctions.2012, numCores = 2)
mwp.2011 = getMarginalWorkPrices(needs.2011, calls.2011, auctions.2011, numCores = 2)

```

```{r}
ggplot(data = data.pos, aes(x=marginal_work_price)) +
  geom_histogram(aes(y = (..count..)/sum(..count..)), breaks = seq(0, 100, 1),
                 col="green",
                 fill="green",
                 alpha = .5) +
  labs(title="Histogram for Marginal Work Prices with positive Reserve Power in 2016") +
  labs(x="Marginal Work Price", y="Count")

```






