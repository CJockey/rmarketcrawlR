---
title: "Analyzing the German Operating Reserve Market"
output: html_notebook
fig_width: 6 
fig_height: 4
---




## Data Mining Process

For the following analysis we will apply the CRISP-DM process as seen in the image below. First it is important to precisely define the goal and problem set. Therefore one has to understand the business needs and processes involved. Based on that, the necessary data can be acquired and prepared for the model building step.


![1. CRISP-DM Process](/Users/Timo/Documents/Studium/2.\ Master/Masterarbeit/\[3\]_Ausarbeitung/LaTex/GER_version/1_graphics/crisp_dm.png){ width=42% }


## Busines Understanding

### Determine Business Objectives

First, to understand the purpose and to be able to interpret the results of the analysis, background information of the business domain is provided in the following section. Hereby, the business objectives for the analysis are defined as well as success criterias to meassure the fulfillment of the goals.

#### Background


##### Secondary Reserve Power

The German Reserve Market distinguishes between three different product types (19/24 european markets follow this pattern [source](http://www.strommarkttreffen.org/2016-1-22-Ocker-German-Secondary-Balancing-Power-Market.pdf.pdf)):

- **Primary Reserve power (PR)**
- **Secondary Reserve Power (SR)**
- **Tertiary Reserve Power (TR)**

Their differences concern the time aspect of power delivery. The image underneath shows the activation duration for full power and the activation time when full power is available. 


![2. The 'three-quality' pattern (PR/SR/TR) in the German Reserve Market](/Users/Timo/Documents/Studium/2.\ Master/Masterarbeit/\[3\]_Ausarbeitung/LaTex/GER_version/1_graphics/rl_timeline.png){ width=50% }

The focus of this analysis lies on the Secondary Reserve Power since its activation time is reasonable for common energy suppliers. The market for SR is also the biggest. 




##### German Reserve Market

Along with the ... law in 2009 the four german TSOs joined and control the German Reserve Market. Their duty for transparency led to the plattform [regelleistung.net](https://www.regelleistung.net). There all data around the reserve market is published. Also the auction results of the SR. Those auctions are held weeklys from monday till wednesday. The offers account for one week (monday till sunday) and are divided in main period and sub period as well as negative and positive SR. An offer consists of a power price, a work price and the offered SR (negative or positive and its amount in MW). 




##### Scenario: Facility 



#### Objective

The overall aim is to give a more precise estimation for the call probability of working prices of the secondary reserve power. Those probabilities can be used as an input paramter for an optimization algorithm which controlls the energy purchasing and selling flow in both primary energy market and reserve energy market. By now the optimal working prices are set to exceed the contribution margin.


#### Success Criterias




### Assess situation

Satus Quo, Resources, Requirements, Assumptions, Constraints, Terminology







### Determine Data mining Goals

Goals, success criterias









### Data Understanding

Next, we will focus on the understanding of the data. As mentioned previously, the main source is the transparent internet platform [regelleistung.net](https://www.regelleistung.net)

#### Collect Data

A R package exists `rmarketcrawlR` accessable via [github](https://github.com/wagnertimo/rmarketcrawlR) which allows crawiling the platform. It also provides additional functions to approximate minutely secondary reserve calls and hence calculate marginal work prices. 
To get started with the package use `devtools` and its `Ã¬nstall_github()` function.

```{r}
library(devtools)
install_github("wagnertimo/rmarketcrawlR")
```

Than you are able to activate the library and aquire the secondary reserve powerd data. We will focus on the year 2016 and get the results of the auctions, the 15 minute reserve calls and the 4 seconds reserve needs. The values rely to the *Netzregelverbund* , a composition of the four german TSOs. After all the data is crawled from the internet, the marginal work prices can be computed by approximating minutely reserve calls.

```{r}
# Activate the package in the workspace
library(rmarketcrawlR)

# Set logging true to be able to comprehend the steps or errors
setLogging(TRUE)

# Get the data of 2016 for operating needs, calls and auctions of secondary reserve power from the Netzregelverbund
needs.2016 = getReserveNeeds('01.01.2016', '31.01.2016')
calls.2016 = getReserveCalls('01.01.2016', '31.01.2016', '6', 'SRL')
auctions.2016 = getReserveAuctions('01.01.2016', '31.01.2016', '2')

# Calculate the marginal work prices for 2016 with 2 cores --> This step takes a while
mwp.2016 = getMarginalWorkPrices(needs.2016, calls.2016, auctions.2016, numCores = 2)
```


For further informations about the  `rmarketcrawlR` package read the `README.md` and the documentation.




#### Describe Data

##### Data Sets and Features

**WHY NETZREGELVERBUND DATA???**

The whole time period: `2011-07-01 00:00:00 CEST` till `2016-12-31 23:59:00 CET`. These are 5 and a half years of data.


##### Approximation of minutely calls




##### Calculation of marginal work prices and call probabilities






##### Missing Values

Missing values occur for 15 minute calls of negative and positive SR of the *Netzregelverbund* in 2013, 2012 and 2011:

- **2013:** There are 37 observations in 2013 with missing values. This is around 0.11%. The main date is `2013-12-04`. At `15:45` till the end of the day values for negative and positive SR calls are missing. Then there are four other days (`2013-04-03`, `2013-06-04`, `2013-09-18` and `2013-12-07`) with one observation each.

- **2012:** There are 38 observations in 2012 with missing values. This is around 0.11%. The main dates are `2012-02-26` and `2012-10-27`. with missing values for negative and positive SR calls in the evening and morning hours. Then there are two other days (`2012-08-05`, `2012-09-09`) with one observation each.

- **2011:** There are 235 observations in 2011 (only half the year, starting from `2011-07-01`) with missing values. This is around 1.33%. The dates are too much to list them.



```{r}
# FOR 2013
nrow(calls.2013[is.na(calls.2013$neg_MW), ])
# [1] 37 --> ca. 0.11%
calls.2013[is.na(calls.2013$neg_MW), "DateTime"]
# [1] "2013-04-03 19:00:00 CEST" "2013-06-04 15:00:00 CEST"
# [3] "2013-09-18 08:30:00 CEST" "2013-12-04 15:45:00 CET" 
# [5] "2013-12-04 16:00:00 CET"  "2013-12-04 16:15:00 CET" 
# [7] "2013-12-04 16:30:00 CET"  "2013-12-04 16:45:00 CET" 
# [9] "2013-12-04 17:00:00 CET"  "2013-12-04 17:15:00 CET" 
# [11] "2013-12-04 17:30:00 CET"  "2013-12-04 17:45:00 CET" 
# [13] "2013-12-04 18:00:00 CET"  "2013-12-04 18:15:00 CET" 
# [15] "2013-12-04 18:30:00 CET"  "2013-12-04 18:45:00 CET" 
# [17] "2013-12-04 19:00:00 CET"  "2013-12-04 19:15:00 CET" 
# [19] "2013-12-04 19:30:00 CET"  "2013-12-04 19:45:00 CET" 
# [21] "2013-12-04 20:00:00 CET"  "2013-12-04 20:15:00 CET" 
# [23] "2013-12-04 20:30:00 CET"  "2013-12-04 20:45:00 CET" 
# [25] "2013-12-04 21:00:00 CET"  "2013-12-04 21:15:00 CET" 
# [27] "2013-12-04 21:30:00 CET"  "2013-12-04 21:45:00 CET" 
# [29] "2013-12-04 22:00:00 CET"  "2013-12-04 22:15:00 CET" 
# [31] "2013-12-04 22:30:00 CET"  "2013-12-04 22:45:00 CET" 
# [33] "2013-12-04 23:00:00 CET"  "2013-12-04 23:15:00 CET" 
# [35] "2013-12-04 23:30:00 CET"  "2013-12-04 23:45:00 CET" 
# [37] "2013-12-07 20:00:00 CET"

# FOR 2012
nrow(calls.2012[is.na(calls.2012$neg_MW), ])
# [1] 38 --> ca. 0.11%
calls.2012[is.na(calls.2012$neg_MW), "DateTime"]
# [1] "2012-02-26 20:30:00 CET"  "2012-02-26 20:45:00 CET" 
# [3] "2012-02-26 21:00:00 CET"  "2012-02-26 21:15:00 CET" 
# [5] "2012-02-26 21:30:00 CET"  "2012-02-26 21:45:00 CET" 
# [7] "2012-02-26 22:00:00 CET"  "2012-02-26 22:15:00 CET" 
# [9] "2012-02-26 22:30:00 CET"  "2012-02-26 22:45:00 CET" 
# [11] "2012-02-26 23:00:00 CET"  "2012-02-26 23:15:00 CET" 
# [13] "2012-02-26 23:30:00 CET"  "2012-02-26 23:45:00 CET" 
# [15] "2012-08-05 00:00:00 CEST" "2012-09-09 00:00:00 CEST"
# [17] "2012-10-27 00:00:00 CEST" "2012-10-27 00:15:00 CEST"
# [19] "2012-10-27 00:30:00 CEST" "2012-10-27 00:45:00 CEST"
# [21] "2012-10-27 01:00:00 CEST" "2012-10-27 01:15:00 CEST"
# [23] "2012-10-27 01:30:00 CEST" "2012-10-27 01:45:00 CEST"
# [25] "2012-10-27 02:00:00 CEST" "2012-10-27 02:15:00 CEST"
# [27] "2012-10-27 02:30:00 CEST" "2012-10-27 02:45:00 CEST"
# [29] "2012-10-27 03:00:00 CEST" "2012-10-27 03:15:00 CEST"
# [31] "2012-10-27 03:30:00 CEST" "2012-10-27 03:45:00 CEST"
# [33] "2012-10-27 04:00:00 CEST" "2012-10-27 04:15:00 CEST"
# [35] "2012-10-27 04:30:00 CEST" "2012-10-27 04:45:00 CEST"
# [37] "2012-10-27 05:00:00 CEST" "2012-10-27 05:15:00 CEST"

# FOR 2011
nrow(calls.2011[is.na(calls.2011$neg_MW), ])
# [1] 235 --> ca. 1.33%
calls.2011[is.na(calls.2011$neg_MW), "DateTime"]
# (..)

```


**Impute Missing Values**

For time series analysis on such a data set, it is not appropriate to leave them out. The amount of proportion is quite less, but nevertheless it is better to approximate the missing values. There would be three different approaches:

- **Use the data of other years:** Since there is no intersection with the missing dates, it is possible to impute the missing values with the values of the year before. Only problem is that for 2011 (also for february 2012) no data of 2010 (february 2011) is available. Another option would be to take an average of the *future* data of the years after.
- **Use reserve needs:** It is also possible to use the 4sec reserve needs which are also used for the minutely approximation of the calls.
- **Use TSO data:** On the given date of missing values, some TSO show SR call data. So one approach would be to sum up those observations and use it to impute the missing values for the *Netzregelverbund*. 


We will go for the latter. Taking the reserve needs would cause a big deviation, since the needs and actual calls differ remarkably. Averaging future years would lead to missinterpretations. Using TSO data seems natural because the summation of the four yield into the *Netzregelverbund* variable. It is also noticed that only one (only for 2011 at `2011-10-30 23:00:00` for one hour 3 TSO lack with data) TSO has missing values for the dates of interest which should keep the error small. This leaves the question how to impute the missing value of that TSO. A simple approach would be to take the value of the day before(this is possible because the first observation has no missing value(`2011-07-01`)). 

```r
# TODO!! --> Compare the deviations of both approaches in comparison with the 15min calls of the Netzregelverbund.
# 1. Calculate first approach (Average of TSOs) and compute the deviations between data of TSOs and *Netzregelverund*
# 2. Calculate the deviations of 15min avg 4sec needs with 15min calls
# 3. Compute the 15min calls out of the 4sec needs


```

#### Explore Data

##### Auctions Data

Firstly, we will investigate the auction results for the SR in 2016. Since we are interested in the work prices, we are going to plot the average work price bid for each week seperated by main and sub period as well as negative and positive SR.

```{r}
# filter by direction
# and get the average of the work price offers
auctions.2016 %>%
  group_by_(.dots = c("date_from", "Direction")) %>%
  summarise(avg = mean(work_price)) %>%
  ggplot(aes(x = date_from, y = avg, colour = Direction)) + 
        geom_line()

```



##### Marginal Work Prices



```{r}

ggplot(data = data.pos, aes(x=marginal_work_price)) +
  geom_histogram(aes(y = (..count..)/sum(..count..)), breaks = seq(0, 100, 1),
                 col="green",
                 fill="green",
                 alpha = .5) +
  labs(title="Histogram for Marginal Work Prices with positive Reserve Power in 2016") +
  labs(x="Marginal Work Price", y="Count")

```






